{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchviz"
      ],
      "metadata": {
        "id": "HEIjI4XnCgXP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "azyzZpkrHBsy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Computational Graphs Overview**\n",
        "\n",
        "Building a simple computational graph involves addressing three fundamental tasks:\n",
        "\n",
        "1. **Tracking Changes in Variables**  \n",
        "   Determine an efficient way to monitor and log changes to a variable as operations are applied.\n",
        "\n",
        "2. **Storing Change Information**  \n",
        "   Devise a structure to store this information in an organized and accessible manner for efficient computation and debugging.\n",
        "\n",
        "3. **Calculating Derivatives**  \n",
        "   Implement logic to compute derivatives for each operation applied to the variables, enabling backpropagation for gradient calculations."
      ],
      "metadata": {
        "id": "O6gwYa1pzd1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note About This Code**\n",
        "\n",
        "This code is meant to be a simple demonstration of how computational graphs and automatic differentiation work. It’s written with the goal of being easy to follow and understand, not to handle every possible case or edge scenario.\n",
        "\n",
        "#### **A Few Things to Keep in Mind**:\n",
        "- It’s designed for learning, not production use, so some situations (like dividing by zero or reusing the same variables again) might not work as expected.\n",
        "- There might be bugs or unexpected behavior in certain cases because the focus is on showing the basic ideas rather than covering every corner.\n",
        "\n",
        "#### **Why Keep It Simple?**\n",
        "The main idea is to help you see how the core concepts work under the hood. It’s a great way to understand the foundations before diving into larger, more complex frameworks like PyTorch or TensorFlow. The code should give you a good starting point for experimenting and learning without getting lost in unnecessary complexity."
      ],
      "metadata": {
        "id": "Zfyl3EjXcrcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manual Approach**\n",
        "\n",
        "Let's explore how derivatives are calculated manually. In the next few code blocks, we will:\n",
        "\n",
        "1. Compute partial derivatives for variables after performing a series of operations.\n",
        "2. Use PyTorch's built-in functionality to observe its results.\n",
        "3. Perform our own calculations to understand why PyTorch produces those results and how we can replicate the process step by step.\n",
        "\n",
        "This approach will give us a deeper understanding of how automatic differentiation works under the hood."
      ],
      "metadata": {
        "id": "XWIoygHH8nNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PyTorch Example**\n",
        "\n",
        "In this example, we create two variables and perform a series of operations on them. Afterward, we call `.backward()` on the resulting variable to compute the gradients of the initial variables with respect to the output. This demonstrates PyTorch's automatic differentiation system in action, showing how it efficiently tracks operations and calculates gradients.\n",
        "\n",
        "Additionally, we use the `torchviz` library to visualize the computational graph. This provides a clear representation of the sequence of operations, helping us understand how the calculations are structured internally."
      ],
      "metadata": {
        "id": "2Yufi-Wy8sEU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "-pPCdUVxm4Rg",
        "outputId": "10ae110e-fab2-482f-e587-6c61c1d748eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"222pt\" height=\"393pt\"\n viewBox=\"0.00 0.00 222.00 393.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 389)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-389 218,-389 218,4 -4,4\"/>\n<!-- 139324015027328 -->\n<g id=\"node1\" class=\"node\">\n<title>139324015027328</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 139324019779968 -->\n<g id=\"node2\" class=\"node\">\n<title>139324019779968</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n</g>\n<!-- 139324019779968&#45;&gt;139324015027328 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139324019779968&#45;&gt;139324015027328</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n</g>\n<!-- 139324015752144 -->\n<g id=\"node3\" class=\"node\">\n<title>139324015752144</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-141 62,-141 62,-122 151,-122 151,-141\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">SubBackward0</text>\n</g>\n<!-- 139324015752144&#45;&gt;139324019779968 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139324015752144&#45;&gt;139324019779968</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106.5,-121.75C106.5,-114.8 106.5,-104.85 106.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110,-96.09 106.5,-86.09 103,-96.09 110,-96.09\"/>\n</g>\n<!-- 139324015750608 -->\n<g id=\"node4\" class=\"node\">\n<title>139324015750608</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"98,-196 9,-196 9,-177 98,-177 98,-196\"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 139324015750608&#45;&gt;139324015752144 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139324015750608&#45;&gt;139324015752144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M62.25,-176.75C69.97,-169.03 81.4,-157.6 90.72,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"93.31,-150.64 97.91,-141.09 88.36,-145.69 93.31,-150.64\"/>\n</g>\n<!-- 139324015751424 -->\n<g id=\"node5\" class=\"node\">\n<title>139324015751424</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"95,-251 6,-251 6,-232 95,-232 95,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n</g>\n<!-- 139324015751424&#45;&gt;139324015750608 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139324015751424&#45;&gt;139324015750608</title>\n<path fill=\"none\" stroke=\"black\" d=\"M51,-231.75C51.39,-224.8 51.95,-214.85 52.45,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"55.94,-206.27 53.01,-196.09 48.95,-205.88 55.94,-206.27\"/>\n</g>\n<!-- 139324015745280 -->\n<g id=\"node6\" class=\"node\">\n<title>139324015745280</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-312 0,-312 0,-293 101,-293 101,-312\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-300\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139324015745280&#45;&gt;139324015751424 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139324015745280&#45;&gt;139324015751424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-292.79C50.5,-284.6 50.5,-272.06 50.5,-261.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-261.24 50.5,-251.24 47,-261.24 54,-261.24\"/>\n</g>\n<!-- 139324015027728 -->\n<g id=\"node7\" class=\"node\">\n<title>139324015027728</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-385 23.5,-385 23.5,-354 77.5,-354 77.5,-385\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-361\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 139324015027728&#45;&gt;139324015745280 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139324015027728&#45;&gt;139324015745280</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-353.75C50.5,-344.39 50.5,-332.19 50.5,-322.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-322.02 50.5,-312.02 47,-322.02 54,-322.02\"/>\n</g>\n<!-- 139324015746960 -->\n<g id=\"node8\" class=\"node\">\n<title>139324015746960</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"206,-196 117,-196 117,-177 206,-177 206,-196\"/>\n<text text-anchor=\"middle\" x=\"161.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">PowBackward0</text>\n</g>\n<!-- 139324015746960&#45;&gt;139324015752144 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139324015746960&#45;&gt;139324015752144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M152.42,-176.75C144.41,-169.03 132.54,-157.6 122.88,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"125.04,-145.51 115.41,-141.09 120.19,-150.55 125.04,-145.51\"/>\n</g>\n<!-- 139324015744176 -->\n<g id=\"node9\" class=\"node\">\n<title>139324015744176</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-251 113,-251 113,-232 214,-232 214,-251\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139324015744176&#45;&gt;139324015746960 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139324015744176&#45;&gt;139324015746960</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.17,-231.75C162.91,-224.8 162.53,-214.85 162.2,-206.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"165.7,-205.95 161.82,-196.09 158.7,-206.21 165.7,-205.95\"/>\n</g>\n<!-- 139324015027088 -->\n<g id=\"node10\" class=\"node\">\n<title>139324015027088</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-318 136.5,-318 136.5,-287 190.5,-287 190.5,-318\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 139324015027088&#45;&gt;139324015744176 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139324015027088&#45;&gt;139324015744176</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.5,-286.92C163.5,-279.22 163.5,-269.69 163.5,-261.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167,-261.25 163.5,-251.25 160,-261.25 167,-261.25\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7eb6e6b52650>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-864.]) tensor([288.])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = torch.tensor([6.], requires_grad=True)\n",
        "c = 3*a**3 - b**2\n",
        "d = c**2\n",
        "\n",
        "d.backward()\n",
        "display(make_dot(d))\n",
        "\n",
        "print(a.grad, b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Manual Calculation**\n",
        "\n",
        "Given the following equations:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "a = 2, \\quad b = 6 \\\\\n",
        "c = 3a^3 - b^2 \\\\\n",
        "d = c^2\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "We aim to calculate $\\frac{\\partial d}{\\partial a} $ and $ \\frac{\\partial d}{\\partial b} $ using the chain rule:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial d}{\\partial a} = \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial a}, \\quad\n",
        "\\frac{\\partial d}{\\partial b} = \\frac{\\partial d}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "### **Step-by-Step Derivatives**\n",
        "\n",
        "1. Compute the partial derivatives of each component:\n",
        "\n",
        "   $$\n",
        "   \\frac{\\partial d}{\\partial c} = 2c, \\quad\n",
        "   \\frac{\\partial c}{\\partial a} = 9a^2, \\quad\n",
        "   \\frac{\\partial c}{\\partial b} = -2b\n",
        "   $$\n",
        "\n",
        "2. **Substitute Known Values:**\n",
        "\n",
        "   - Compute \\( c \\):\n",
        "     $$\n",
        "     c = 3(2)^3 - 6^2 = 3(8) - 36 = 24 - 36 = -12\n",
        "     $$\n",
        "   - Compute \\( d \\):\n",
        "     $$\n",
        "     d = (-12)^2 = 144\n",
        "     $$\n",
        "\n",
        "3. **Calculate the partial derivatives:**\n",
        "\n",
        "   - For $ \\frac{\\partial d}{\\partial c} $:\n",
        "\n",
        "     $$\n",
        "     \\frac{\\partial d}{\\partial c} = 2(-12) = -24\n",
        "     $$\n",
        "   - For $ \\frac{\\partial c}{\\partial a} $:\n",
        "\n",
        "     $$\n",
        "     \\frac{\\partial c}{\\partial a} = 9(2)^2 = 9(4) = 36\n",
        "     $$\n",
        "   - For $ \\frac{\\partial c}{\\partial b} $:\n",
        "   \n",
        "     $$\n",
        "     \\frac{\\partial c}{\\partial b} = -2(6) = -12\n",
        "     $$\n",
        "\n",
        "4. **Combine Using the Chain Rule:**\n",
        "\n",
        "   - For $ \\frac{\\partial d}{\\partial a} $:\n",
        "\n",
        "     $$\n",
        "     \\frac{\\partial d}{\\partial a} = (-24) \\cdot 36 = -864\n",
        "     $$\n",
        "   - For $ \\frac{\\partial d}{\\partial b} $:\n",
        "   \n",
        "     $$\n",
        "     \\frac{\\partial d}{\\partial b} = (-24) \\cdot (-12) = 288\n",
        "     $$\n",
        "\n",
        "### **Final Results**\n",
        "\n",
        "The partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "\\frac{\\partial d}{\\partial a} = -864, \\quad \\frac{\\partial d}{\\partial b} = 288\n",
        "}\n",
        "$$"
      ],
      "metadata": {
        "id": "wtAqQhOu8S4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a = 2\n",
        "b = 6\n",
        "c = 3*a**3 - b**2\n",
        "d = c**2\n",
        "\n",
        "dd_dc = 2*c\n",
        "dc_da = 9*a**2\n",
        "dc_db = -2*b\n",
        "\n",
        "d_da = dd_dc * dc_da\n",
        "d_db = dd_dc * dc_db\n",
        "\n",
        "d_da, d_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK6ZE6Ujzm8Y",
        "outputId": "0f5a1b66-c34b-4b0c-f366-c595fcee03b7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-864.], grad_fn=<MulBackward0>),\n",
              " tensor([288.], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Simple Framework**\n",
        "\n",
        "In this section, we will build a simple class to represent variables and operations in a computational graph. This framework will allow us to:\n",
        "\n",
        "1. Track changes made to variables through a series of operations.\n",
        "2. Calculate the derivatives (gradients) of these variables with respect to a final output using the chain rule of calculus.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Does the Framework Work?**\n",
        "\n",
        "The framework is based on the following principles:\n",
        "\n",
        "1. **Variable Representation**:  \n",
        "   Each variable is encapsulated in a `Variable` class that stores:\n",
        "   - Its current value.\n",
        "   - Its gradient (initialized to zero).\n",
        "   - A record of parent variables and the operations that led to its creation.\n",
        "\n",
        "2. **Tracking Operations**:  \n",
        "   When operations like addition, subtraction, multiplication, or division are applied to variables, the framework:\n",
        "   - Creates a new `Variable` instance representing the result.\n",
        "   - Links the result to its parent variables and stores the operation type.\n",
        "\n",
        "3. **Backpropagation and Gradients**:  \n",
        "   Gradients are calculated using the **chain rule**, which states that:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\n",
        "   $$\n",
        "   By recursively traversing the computational graph, we compute the gradient of the final output with respect to each input variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Are Gradients for Each Operation Defined as They Are?**\n",
        "\n",
        "The framework uses predefined gradient rules for common operations. Here's the reasoning behind each:\n",
        "\n",
        "1. **Addition**:  \n",
        "   If $ z = x + y $, the gradient of $ z $ with respect to $ x $ or $ y $ is:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = 1, \\quad \\frac{\\partial z}{\\partial y} = 1\n",
        "   $$\n",
        "   This is because the rate of change of the sum is equal to the rate of change of each individual component.\n",
        "\n",
        "2. **Subtraction**:  \n",
        "   If $ z = x - y $, the gradients are:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = 1, \\quad \\frac{\\partial z}{\\partial y} = -1\n",
        "   $$\n",
        "   The result decreases as $ y $ increases, hence the negative sign.\n",
        "\n",
        "3. **Multiplication**:  \n",
        "   If $ z = x \\cdot y $, the gradients are:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = y, \\quad \\frac{\\partial z}{\\partial y} = x\n",
        "   $$\n",
        "   The rate of change of $ z $ with respect to one variable is proportional to the other variable, as the other variable acts as a coefficient.\n",
        "\n",
        "4. **Division**:  \n",
        "   If $ z = \\frac{x}{y} $, the gradients are:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = \\frac{1}{y}, \\quad \\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}\n",
        "   $$\n",
        "   For $ x $, the result increases as $ x $ increases. For $ y $, the result decreases nonlinearly as $ y $ increases.\n",
        "\n",
        "5. **Exponentiation**:  \n",
        "   If $ z = x^y $, the gradients are more complex:\n",
        "   $$\n",
        "   \\frac{\\partial z}{\\partial x} = y \\cdot x^{y-1}\n",
        "   $$\n",
        "   The gradient with respect to $ x $ follows the power rule.\n",
        "\n",
        "\n",
        "### **Why This Approach Matters**\n",
        "\n",
        "This framework showcases the core logic of autodiff systems by:\n",
        "- **Tracking Dependencies**: Each variable knows which other variables contributed to its creation and how.\n",
        "- **Applying the Chain Rule**: By recursively propagating gradients, we can compute how changes in any variable affect the final output.\n",
        "- **Simplicity**: The minimalist implementation provides a clear foundation for understanding more complex frameworks like PyTorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "5mLUmD-E82Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Class Definition**\n",
        "\n",
        "In this class, we override Python's default magic methods, such as `__add__`, `__sub__`, `__mul__`, `__truediv__`, and `__pow__`. These methods are automatically invoked when we use the `+`, `-`, `*`, `/`, and `**` operators, respectively.\n",
        "\n",
        "By overriding these methods, we retain their original functionality while introducing additional steps to:\n",
        "\n",
        "1. **Track Operations**:  \n",
        "   Each operation is logged by recording the parent variables and the type of operation. This dynamic logging constructs the computational graph as the operations are performed.\n",
        "\n",
        "2. **Create Resultant Variables**:  \n",
        "   The result of each operation is encapsulated in a new `Variable` instance. This instance contains references to its dependencies (parent variables) and the operation that created it, enabling us to trace the computation.\n",
        "\n",
        "3. **Enable Gradient Calculation**:  \n",
        "   The recorded operations allow us to compute gradients through backpropagation. Each operation's specific rules determine how the gradients of the parent variables are computed.\n",
        "\n",
        "For **asymmetrical operations** (where the derivative with respect to the left operand differs from that of the right operand), both methods must be explicitly handled to ensure correct gradient computation. For example:\n",
        "\n",
        "- **Subtraction (`-`)**:\n",
        "  - The left operand's derivative is `1` (positive change).\n",
        "  - The right operand's derivative is `-1` (negative change).\n",
        "  - This requires separate implementations for `__sub__` and `__rsub__`.\n",
        "\n",
        "- **Division (`/`)**:\n",
        "  - The left operand's derivative is $\\frac{1}{\\text{denominator}}$.\n",
        "  - The right operand's derivative is $-\\frac{\\text{numerator}}{\\text{denominator}^2}$.\n",
        "  - These differences are addressed by defining both `__truediv__` and `__rtruediv__`."
      ],
      "metadata": {
        "id": "BW6xAe7zHaZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRADIENT_RULES = {\n",
        "            'add'  : lambda self, parent, other_parent: 1,\n",
        "            'sub'  : lambda self, parent, other_parent: 1,\n",
        "            'rsub' : lambda self, parent, other_parent: -1,\n",
        "            'mul'  : lambda self, parent, other_parent: other_parent.value,\n",
        "            'div'  : lambda self, parent, other_parent: 1 / other_parent.value,\n",
        "            'rdiv' : lambda self, parent, other_parent: -1 * (other_parent.value/parent.value**2),\n",
        "            'pow'  : lambda self, parent, other_parent: other_parent.value * (parent.value ** (other_parent.value - 1))\n",
        "}\n",
        "\n",
        "\n",
        "class Variable:\n",
        "    def __init__(self, value, name=None):\n",
        "        self.value = value\n",
        "        self.grad = 0\n",
        "        self.parents = []\n",
        "        self.name = name\n",
        "\n",
        "    def add_parent(self, parent, op):\n",
        "        \"\"\"Add a parent and the operation that produced this variable.\"\"\"\n",
        "        self.parents.append((parent, op))\n",
        "\n",
        "    def _get_other_parent(self, parent):\n",
        "        for other_parent, _ in self.parents:\n",
        "            if other_parent != parent:\n",
        "                return other_parent\n",
        "        return None, None\n",
        "\n",
        "    def __repr__(self):\n",
        "        parent_info = \", \".join([f\"{p.name} ({op})\" for p, op in self.parents])\n",
        "        return (\n",
        "            f\"Variable(value={self.value}, grad={self.grad}, name={self.name}, \"\n",
        "            f\"parents=[{parent_info}])\"\n",
        "        )\n",
        "\n",
        "    def __add__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(self.value + other.value)\n",
        "        result.add_parent(self, 'add')\n",
        "        result.add_parent(other, 'add')\n",
        "        return result\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        return self.__add__(other)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(self.value - other.value)\n",
        "        result.add_parent(self, 'sub')\n",
        "        result.add_parent(other, 'rsub')\n",
        "        return result\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(other.value - self.value)\n",
        "        result.add_parent(other, 'sub')\n",
        "        result.add_parent(self, 'rsub')\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(self.value * other.value)\n",
        "        result.add_parent(self, 'mul')\n",
        "        result.add_parent(other, 'mul')\n",
        "        return result\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        return self.__mul__(other)\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(self.value / other.value)\n",
        "        result.add_parent(self, 'div')\n",
        "        result.add_parent(other, 'rdiv')\n",
        "        return result\n",
        "\n",
        "    def __rtruediv__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(other / self.value)\n",
        "        result.add_parent(other, 'div')\n",
        "        result.add_parent(self, 'rdiv')\n",
        "        return result\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        if not isinstance(other, Variable):\n",
        "            other = Variable(other)\n",
        "\n",
        "        result = Variable(self.value**other.value)\n",
        "        result.add_parent(self, 'pow')\n",
        "        result.add_parent(other, 'pow')\n",
        "        return result\n",
        "\n",
        "    def accumulate_global_gradients(self):\n",
        "        for parent, op in self.parents:\n",
        "            parent.grad = self.grad * parent.grad\n",
        "            parent.accumulate_global_gradients()\n",
        "\n",
        "    def calculate_gradients(self, gradient=1):\n",
        "        self.grad = gradient\n",
        "        for parent, op in self.parents:\n",
        "            parent.calculate_gradients(GRADIENT_RULES[op](self, parent, self._get_other_parent(parent)))\n",
        "\n",
        "    def backpropagate(self):\n",
        "        self.calculate_gradients()\n",
        "        self.accumulate_global_gradients()"
      ],
      "metadata": {
        "id": "ON3JU01Z8g8u"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare with PyTorch"
      ],
      "metadata": {
        "id": "DfYmbgoM-uyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_with_torch(test_name, var_grad, torch_grad):\n",
        "    \"\"\"\n",
        "    Compare custom implementation gradients with PyTorch gradients.\n",
        "    \"\"\"\n",
        "    print(f\"{test_name} - Custom grad: {var_grad}, Torch grad: {torch_grad}\")\n",
        "    assert np.allclose(var_grad, torch_grad), (\n",
        "        f\"Gradients do not match for {test_name}.\\n\"\n",
        "        f\"Expected (Torch): {torch_grad}, Got (Custom): {var_grad}\"\n",
        "    )\n",
        "\n",
        "def test_addition():\n",
        "    a = Variable(2, name=\"a\")\n",
        "    b = Variable(3, name=\"b\")\n",
        "    z = a + b\n",
        "    z.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(2.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(3.0, requires_grad=True)\n",
        "    z_torch = a_torch + b_torch\n",
        "    z_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Addition (a)\", a.grad, a_torch.grad.item())\n",
        "    compare_with_torch(\"Addition (b)\", b.grad, b_torch.grad.item())\n",
        "\n",
        "def test_subtraction():\n",
        "    a = Variable(5, name=\"a\")\n",
        "    b = Variable(3, name=\"b\")\n",
        "    z = a - b\n",
        "    z.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(5.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(3.0, requires_grad=True)\n",
        "    z_torch = a_torch - b_torch\n",
        "    z_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Subtraction (a)\", a.grad, a_torch.grad.item())\n",
        "    compare_with_torch(\"Subtraction (b)\", b.grad, b_torch.grad.item())\n",
        "\n",
        "def test_multiplication():\n",
        "    a = Variable(2, name=\"a\")\n",
        "    b = Variable(6, name=\"b\")\n",
        "    z = a * b\n",
        "    z.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(2.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(6.0, requires_grad=True)\n",
        "    z_torch = a_torch * b_torch\n",
        "    z_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Multiplication (a)\", a.grad, a_torch.grad.item())\n",
        "    compare_with_torch(\"Multiplication (b)\", b.grad, b_torch.grad.item())\n",
        "\n",
        "def test_division():\n",
        "    a = Variable(6, name=\"a\")\n",
        "    b = Variable(2, name=\"b\")\n",
        "    z = a / b\n",
        "    z.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(6.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(2.0, requires_grad=True)\n",
        "    z_torch = a_torch / b_torch\n",
        "    z_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Division (a)\", a.grad, a_torch.grad.item())\n",
        "    compare_with_torch(\"Division (b)\", b.grad, b_torch.grad.item())\n",
        "\n",
        "def test_power():\n",
        "    a = Variable(2, name=\"a\")\n",
        "    b = Variable(3, name=\"b\")\n",
        "    z = a ** b\n",
        "    z.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(2.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(3.0, requires_grad=True)\n",
        "    z_torch = a_torch ** b_torch\n",
        "    z_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Power (a)\", a.grad, a_torch.grad.item())\n",
        "\n",
        "test_addition()\n",
        "test_subtraction()\n",
        "test_multiplication()\n",
        "test_division()\n",
        "test_power()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1JoyDbXqJfB",
        "outputId": "57b563c3-2ac3-4ef5-fcea-1de6765ed803"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Addition (a) - Custom grad: 1, Torch grad: 1.0\n",
            "Addition (b) - Custom grad: 1, Torch grad: 1.0\n",
            "Subtraction (a) - Custom grad: 1, Torch grad: 1.0\n",
            "Subtraction (b) - Custom grad: -1, Torch grad: -1.0\n",
            "Multiplication (a) - Custom grad: 6, Torch grad: 6.0\n",
            "Multiplication (b) - Custom grad: 2, Torch grad: 2.0\n",
            "Division (a) - Custom grad: 0.5, Torch grad: 0.5\n",
            "Division (b) - Custom grad: -1.5, Torch grad: -1.5\n",
            "Power (a) - Custom grad: 12, Torch grad: 12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_complex():\n",
        "    a = Variable(2, name=\"a\")\n",
        "    b = Variable(3, name=\"b\")\n",
        "    c = 3*a**3 - b**2\n",
        "    d = c**2\n",
        "    d.backpropagate()\n",
        "\n",
        "    a_torch = torch.tensor(2.0, requires_grad=True)\n",
        "    b_torch = torch.tensor(3.0, requires_grad=True)\n",
        "    c_torch = 3 * a_torch**3 - b_torch**2\n",
        "    d_torch = c_torch**2\n",
        "    d_torch.backward()\n",
        "\n",
        "    compare_with_torch(\"Complex (a)\", a.grad, a_torch.grad.item())\n",
        "    compare_with_torch(\"Complex (b)\", b.grad, b_torch.grad.item())\n",
        "\n",
        "test_complex()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT6lIHppH2u1",
        "outputId": "58d110d6-f22a-4aae-e2ff-32f7a9c5a908"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex (a) - Custom grad: 1080, Torch grad: 1080.0\n",
            "Complex (b) - Custom grad: -180, Torch grad: -180.0\n"
          ]
        }
      ]
    }
  ]
}